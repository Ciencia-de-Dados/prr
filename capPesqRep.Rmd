---
title: "Pesquisa Reproduzível"
author: "Fernando Mayer & Walmes Zeviani"
output:
  html_document:
    toc: true
---

```{r setup, include=FALSE}
source("setup_knitr.R")
```

# Introdução

Atualmente a evolução da tecnologia, mais especificamente em como
coletamos, processamos e analisamos dados, têm possibilitado a
interpretação de bases de dados complexas e de alta dimensão. Algumas
bases de dados podem ser geradas quase que instantaneamente, se
comparado à alguns anos atrás. Além disso, utilizamos o alto poder
computacional atual para combinar bases de dados existentes (que já
possuem grandes dimensões) em bases de dados cada vez maiores. Também
utilizamos a alta capacidade de processamento para implementar rotinas
de análise cada vez mais sofisticadas e complexas.

Mas quem garante que toda essa informação, e todo esse resultado que vem
sendo gerado pode ser confiável? Não existe uma resposta direta para
essa pergunta, e certamente a confiabilidade de uma informação depende
de muitos fatores. Uma possibilidade é confiarmos na afirmação que
alguém fez, e simplesmente acreditar que isso seja verdade. Você tomaria
um comprimido que alguém está distribuindo na rua afirmando que ele cura
a sua dor de cabeça? E se um médico lhe oferecesse esse mesmo
comprimido, isso faria você tomar o remédio com mais segurança?
Certamente sim, mas quem garante que o médico não possa estar errado e o
remédio causar um efeito colateral devastador em você? A resposta para
essa pergunta é fácil. Considerando que o médico seja responsável, ele
**pesquisou** antes de indicar a medicação para alguém. Mas quem garante
que a pesquisa que o médico se baseou é confiável? É aí que entra a
teoria da ciência e as formas de validação de um
resultado. (*Arrumar essa ideia e continuar...*)

As análises, os modelos e os algoritmos que usamos hoje em dia são
muito mais complexos do que no passado. Ter um entendimento básico
de tudo isso é difícil, até mesmo para os mais qualificados, e é quase
impossível descrever todos os detalhes em palavras apenas. Por isso,
entender o que alguém fez em uma análise de dados atualmente, requer
olhar para o *código*, e examinar minuciosamente as rotinas utilizadas
pelas pessoas.

<div class="panel panel-primary">
  <div class="panel-heading">Um estudo de caso</div>
  <div class="panel-body">

  Em 2006, um artigo no *New England Journal of Medicine*, dos Drs. Anil
  Potti, Joseph Nevins e colaboradores da Universidade de Duke, chamou a
  atenção da comunidade médica em geral. Os autores relataram que
  poderiam prever a evolução do câncer de pulmão de um paciente usando
  dispositivos chamados de matrizes de expressão, que rastreiam os
  padrões de atividade de milhares de genes em uma amostra de tecido
  como uma imagem colorida. Alguns meses mais tarde, eles publicaram na
  *Nature Medicine* que eles tinham desenvolvido uma técnica semelhante,
  que utilizou a expressão de genes em culturas de laboratório de
  células cancerosas, conhecidas como linhas de células, para prever a
  quimioterapia mais eficaz para pacientes que sofrem de câncer de
  pulmão, mama ou de ovário.

  Aparentemente, os resultados publicados pareciam um enorme avanço no
  tratamento do câncer. De fato, algumas semanas depois da publicação
  na *Nature Medicine*, um grupo de bioestatísticos do MD Anderson
  Cancer Centre tentava reproduzir os resultados do artigo, com a
  finalidade de usar a nova técnica. Ao encontrar algumas dificuldades,
  os pesquisadores entraram em contato com Dr. Potti, solcitando os
  dados brutos e os códigos da análise conduzida em Duke.

  Os pesquisadores de Duke forneceram o material, e logo começaram a
  surgir diversos questionamentos sobre a metodologia utilizada
  originalmente. Os pesquisadores que tantavam reproduzir a análise
  encontraram diversas falhas como linhas de células com nomes errados,
  nomes e números de células inconsistentes, etc.

  Os questionamentos foram publicamente respondidos, e além disso, o
  grupo de Duke iniciou três ensaios clínicos baseados nos trabalhos
  publicados. Também havia a intenção de utilizar a metodologia em um
  ensaio clínico financiado pela *America's National Cancer Institute*
  (NCI), o que levou Lisa McShane, uma bioestatística da NCI, também
  tentar reproduzir o trabalho, o que também foi em vão.

  Depois disso, uma série de investigações foram realizadas, inclusive
  por outros pesquisadores da Duke, levando a cessar os ensaios clínicos
  já iniciados pela Universidade. Dr. Potti pediu demissão, e depois foi
  descoberto que ele tinha envolvimento com empresas da área biomédica.

  Como lição: o processo de revisão por pares de revistas científicas
  depende da disponibilidade de pesquisadores, que frequentemente não
  tem o tempo necessário para revisar uma publicação da maneira como
  deveria. Além do mais, a seção "métodos" de um artigo deveria conter
  toda a informação necessária para que a análise seja reproduzida, o
  que frequentemente não acontece.
  </div>
</div>

# Análise de dados como arte

Em 1974, Donald Knuth descreveu a diferença entre arte e ciência da
seguinte forma:

> Ciência é conhecimento que entendemos tão bem que podemos ensiná-la
> para um computador. Todo o resto é arte.

Imagine um compositor escrevendo a letra de uma música. Certamente
existem regras gerais de como uma boa música deve ser estruturada: qual a
duração, número de versos, etc. Em outras palavras, existe um estrutura
geral (mesmo que abstrata) para músicas em geral. No entanto, o
conhecimento dessa estrutura e de teoria musical sozinhas não fazem uma
boa música. É necessário algo mais. Em algum momento, o compositor deve
injetar algo mais, alguma coisa que faça com que sua música seja
diferente das outras e que as pessoas queiram ouvir. Esta parte criativa
é difícil de explicar, mas é essencial para o processo de composição. Se
não fosse assim, bastaria programarmos um computador para compor
músicas, o que (para o bem ou para o mal) ainda não aconteceu.

Assim como a composição de uma música, o processo de analisar dados
também é uma arte. Não é uma coisa que podemos ensinar para um
computador. Análise de dados também possui uma estrutura geral e teorias
envolvidas como regressão linear, árvores de classificação, etc., e
todas estas teorias já foram ensinadas para os computadores. No entanto,
cabe ao analista de dados saber como juntar todas estas ferramentas e
aplicá-las para responder questões relevantes para a ciência e para as
pessoas. (Peng e Matsui, 2015).

# Como a ciência funciona

A Ciência é um processo de aprendizado da natureza, onde ideias
concorrentes sobre como funciona o mundo são medidas contra observações
(Feynman, 1965, 1985). Como as nossas descrições do mundo são quase
sempre incompletas, e nossas medidas envolvem incerteza e imprecisão,
nós precisamos de métodos para avaliar a concordância das ideias
concorrentes e as observações. Estes métodos geralmente constituem o
campo da Estatística (Stigler, 1986).

Existe um modelo comumente aceito para o processo científico. A ideia
geral é de que existe uma "árvore de aprendizado" a partir de
experimentos críticos, denominada por Platt (1964) de **inferência
forte** (*strong inference*), e que consiste das seguintes etapas:

1. Conceber hipóteses alternativas.
2. Conceber um experimento crucial (ou vários deles) com possíveis
   resultados alternativos, onde cada um poderá excluir, dentro do
   possível, uma ou mais hipóteses.
3. Realizar o experimento de forma a obter resultados mais confiáveis
   possíveis.
4. Reciclar o procedimento, criando subhipóteses ou hipóteses
   sequenciais para refinar as possibilidades que restam, e assim por
   diante.

Esta visão de Platt é naturalmente uma extensão lógica do trabalho de
Popper (1979), que revolucionou a filosofia da Ciência no século 20, ao
argumentar que uma hipótese não pode ser provada, apenas desprovada. A
essência do método Popperiano é "desafiar" uma hipótese repetidamente.
Se a hipótese permanece válida então ela não é validada, mas adquire um
certo "grau de confiança", que na prática passa a ser tratada como
verdadeira. Coincidindo com esta filosofia de Popper está o trabalho
estatístico de Ronald Fisher, Karl Perason, Jerzy Neyman e outros, que
desenvolveram grande parte da teoria estatística atual associada à
"testes de hipótese".

A etapa 4 do modelo acima é uma das partes fundamentais da Ciência,
tanto para verificar e validar resultados, quanto para criar novas
hipóteses a partir de hipóteses correntes. E não necessariamente esta
etapa deve ser conduzida sempre pelas mesmas pessoas. Muitas vezes,
pesquisadores com diferentes visões e interpretações é que podem fazer
essa etapa evoluir de maneira mais rápida, ou mais profunda. Mas para
isso, é necessário primeiro que a pesquisa original seja **replicável**
por outros cientistas. E é nesse ponto que podem surgir dificuldades,
com será exposto abaixo.

# Replicação e reprodução na Ciência

<!-- Peng -->

Para que um resultado de uma pesquisa científica possa ser considerado
como válido, é necessário que este resultado possa ser **replicado** por
diversos cientistas. A replicação é um dos pilares fundamentais da
ciência. Basicamente é necessário que diversos cientistas coletem e
analisem dados de forma independente, e cheguem no mesmo
resultado. Dessa forma se você chegar à conclusão de que X causa Y, ou
que a vitamina C melhora a doença Z, ou que alguma coisa causa algum
problema, será necessário que outros cientistas, independentes de você,
cheguem no mesmo resultado. Se muitas pessoas diferentes chegarem à
mesma conclusão de forma independente, então tendemos a pensar que a
relação ou resultado provavelmente é verdadeiro (que vai de encontro com
a filosofia de Popper).

A replicação na ciência têm sido praticada há muitos anos. No entanto,
hojem em dia esta prática tem se tornado cada vez mais desafiadora, uma
vez que os estudos estão se tornando cada vez maiores e mais caros, e a
disponibilidade de recursos financeiros para pesquisas cada vez mais
escassa. Além disso, existem estudos que dificilmente podem ser
replicados, como por exemplo, a avaliação do impacto de um terremoto em
um determinado local, a evolução do crescimento de uma floresta, ou um
estudo clínico que acompanhou as reações de pacientes à um medicamento
durante 20 anos.

Existem muitas boas razões pelas quais não podemos replicar um
estudo. Se replicar não é possível, então a opção seria não fazer nada,
e ter como informação apenas o resultado de *um* estudo. No entanto, não
fazer nada não é (e não pode ser) uma opção para cientistas. É nesse
ponto que entra o conceito de **pesquisa reproduzível**. A ideia é criar
uma espécie de "padrão mínimo", ou um "meio-termo" entre replicar um
estudo e não fazer nada, ou seja, fazer algo no meio. O problema básico
é que temos o padrão ideal, que é a replicação, e o padrão nulo, que é não
fazer nada. O que pode ser feito entre o melhor (que não é possível) e o
pior (que não é desejável), é a reprodução dos resultados. É uma forma
de preencher a lacuna de informação entre replicação e nada.

A diferença entre replicação e reprodução está esquematizada na figura
abaixo.

```
Figura com esquema de replicação e reprodução

##------------------------------------------------------------------------
## Replicação

+------------------+
|                  |                         +------------------+
|      Estudo      |                         |                  |
|        1         | +-------------------->  |   Resultado X    |
|      Dados       |                         |                  |
|                  |                         +------------------+
+------------------+
+------------------+
|                  |                         +------------------+
|      Estudo      |                         |                  |
|        2         | +-------------------->  |   Resultado X    |
|      Dados       |                         |                  |
|                  |                         +------------------+
+------------------+
+------------------+
|                  |                         +------------------+
|      Estudo      |                         |                  |
|        3         | +-------------------->  |   Resultado X    |
|      Dados       |                         |                  |
|                  |                         +------------------+
+------------------+

##------------------------------------------------------------------------
## Reprodução

                             +---------------+
                             |  Resultado X  |
                     +-----> |               |
                     |       +---------------+
                     |
+------------------+ |
|                  | |                      +-------------------+
|      Estudo      | |                      |                   |
|        1         | +--------------------> |    Resultado X    |
|      Dados       | |                      |                   |
|                  | |                      +-------------------+
+------------------+ |
                     |
                     |        +--------------+
                     +------> |  Resultado X |
                              |              |
                              +--------------+
```

Uma pesquisa reproduzível é aquela que pode ser reproduzida por outras
pessoas de maneira independente, com o objetivo de que todas cheguem ao
mesmo resultado. Por isso, uma parte fundamental da pesquisa
reproduzível é tornar disponíveis os **dados** do estudo original, e os
**métodos computacionais** (em forma de código) utilizados para se
chegar no resultado. Dessa forma, qualquer pessoa poderá olhar para os
dados, realizar a análise proposta, e chegar exatamente nos mesmos
resultados.

Nesse ponto, a pergunta é: se reproduzir uma análise leva à um resultado
já conhecido e esperado, então qual seria o propósito de se fazer (ou de
tornar) uma pesquisa reproduzível? Como mencionado anteriormente, a
pesquisa reproduzível não é uma forma de replicação, mas é melhor do que
não se fazer nada. O propósito principal de reproduzir uma pesquisa é o
de **validação da análise de dados**. Como não há coleta de dados e
métodos analíticos independentes, realmente é difícil validar a questão
científica propriamente dita. Mas, se conseguimos reproduzir uma análise
e chegar no mesmo resultado do autor original, então de alguma forma
estamos validando aquela análise de dados. Com isso, no mínimo podemos
ter confiança de que aquela análise foi realizada apropriadamente e que
os métodos corretos foram utilizados.

Além disso, a possibilidade de reproduzir um estudo com os dados
originais também permite que diferentes cientistas, com diferentes
visões e ideias possam colaborar no sentido de continuar a pesquisa, ou
sugerir outras abordagens de análise, ou abrindo novas possibilidades
para novas perguntas. Dessa forma, a pesquisa reproduzível deve ser
vista como um processo dinâmico, e não estático, onde o fim seria chegar
no resultado esperado. O verdadeiro trunfo da pesquisa reproduzível é
quando novas ideias e perguntas surgem a partir dos resultados
encontrados, e a disponibilização de dados e códigos podem fazer com que
o processo de geração de conhecimento (que é um dos objetivos da
ciência) seja mais rápido e dinâmico. No futuro, podem existir outras
abordagens e formas de pensar no mesmo problema, e nesse sentido, novos
métodos podem ser utilizados para analisar os dados disponibilizados. Se
o resultado encontrado for sempre o mesmo, também tendemos a acreditar
que deve ser verdadeiro.

Nos últimos anos têm havido muita discussão sobre pesquisa reproduzível
na literatura científica e na mídia. A revista *Science* teve uma edição
inteira dedicada à reproducibilidade. Muitas revistas científicas tem
atualizado suas políticas de publicação, para encorajar a
reproducibilidade dos artigos publicados. Por exemplo, a *Public Library
of Science* (PLoS) possui como requerimento para publicação a
disponibilização *online* dos dados necessários para reproduzir uma
pesquisa.

Nesse ponto podem surgir algumas preocupações por parte dos cientistas:
Como fica a questão da propriedade intelectual? Como fica a
disponibilização de dados que foram coletados com financiamento privado
ou público? Até que ponto a disponibilização dos dados não irá
prejudicar meus projetos futuros? Será que não posso sofrer plágio?

Muitas pesquisas tem mostrado que os cientistas estão preocupados em
disponibilizar seus dados em detrimento de pesquisas futuras. Na própria
PLoS, cerca de 60% das publicações não disponibilizam dados suficientes
para que as pesquisas possam ser reproduzidas. (Van Noorden, 2014).

Como lidar e argumentar contra isso?

# Elementos da reproducibilidade

Quando lemos um artigo ou relatório científico, na maioria das vezes, o
que temos disponível é um texto e nada mais. Certamente sabemos que por
trás daquele texto houve *muito* trabalho envolvido. O autor segue a via
da pergunta científica para o texto publicado, e o leitor faz o caminho
inverso. A ideia básica por traz da reproducibilidade é focar nos
**dados** e **métodos computacionais**, que se disponíveis, farão o
autor e o leitor se "encontrarem no meio do caminho".

Então o que é necessário para a reproducibilidade? Podem existir várias
maneiras de descrever essa necessidade, mas basicamente existem quatro
elementos principais:

1. **Dados**: Os dados utilizados na análise devem ser disponibilizados
   *da maneira como foram analisados originalmente*. Se dados brutos
   foram manipulados (em termos de organização, *subsets*, etc.), então
   deve haver uma *rotina* explicando como chegar nos dados utilizados a
   partir dos dados brutos.
2. **Código**: O código utilizado para produzir os resultados
   apresentados. Isso inclui códigos de pré-processamento de dados (ver
   item acima), análise estatística e geração de gráficos e tabelas.
3. **Documentação**: Descrever o **código** e os **dados** utilizados na
   análise de maneira clara.
4. **Distribuição**: Distribuir todos esses elementos de maneira que
   sejam facilmente acessíveis.

# Boas práticas para a computação científica

Cientistas passam cada vez mais tempo construindo e utilizando
*software*. No entanto, a maioria nunca foi ensinada como fazer isso de
maneira eficiente. Como resultado, muitos desconhecem as ferramentas e
práticas que os fariam escrever códigos mais consistentes e fáceis de
manter, com menor esforço.

Estudos recentes tem mostrado que cientistas passam cerca de 30% de seu
tempo escrevendo códigos. No entanto, mais de 90% deles são
primariamente auto-didatas, e portanto carecem de exposição à boas
práticas de desenvolvimento de *software* como: escrever códigos de
fácil manutenção, usar um *sistema de controle de versões*, rastreadores
de *bugs*, testes unitários, e automação de tarefas.

Para promover, encorajar, e padronizar a reproducibilidade de
investigações científicas, algumas práticas podem ser adotadas. De
maneira específica, podemos listar oito práticas consideradas essenciais
para que a reproducibilidade seja eficiente:

1. Escreva programas para humanos, não para computadores
    a. Crie nomes consistentes, distintos e que possuam significado
    b. A formatação e estilo de código deve ser consistente
2. Deixe o computador fazer o trabalho
    a. Crie funções para fazer tarefas repetitivas
    b. Use uma ferramenta de "construção" (`make` ou
    `rmarkdown::render()`) para automatizar *workflows*
3. Faça alterações incrementais
    a. Trabalhe em pequenos passos e frequentemente revise o que foi
    feito
    b, Use um sistema controlador de versões
4. Não repita você mesmo (ou outros)
    a. Faça o código modulável ao invés de copiar e colar
    b. Re-utilize código ao invés de reescrevê-lo
5. Prepare-se para erros
    a. Use uma plataforma de testes independente
    b. Transforme *bugs* (ou **resultados negativos**) em estudos de
    caso
6. Otimize código apenas depois que ele funcionar
    a. Primeiro a ideia, depois a otimização
    b. Escreva de maneira mais simples possível desde que não perca a
    eficiência
7. Documente a ideia e o propósito, não a mecânica
    a. Descreva motivos e razões, não implementações
    b. De preferência, documente o processo com códigos *embutidos* em
    texto
8. Colabore
    a. Use "programação em pares"
    b. Use um sistema de rastreamento de *bugs* e *issues*

(*Baseada em Wilson et al, 2014*).

# Ferramentas da reproducibilidade

É importante entender que existem autores querendo que sua pesquisa
seja reproduzível, e também que existem leitores que querem reproduzir
pesquisas. Todos precisam de **ferramentas** para facilitar esse
processo.

Publicar dados e códigos não é necessariamente uma tarefa trivial. De
fato, é necessário um grande esforço para fazer com que resultados
cheguem à uma grande audiência. Embora existam diversos recursos
disponíveis atualmente (que não estavam disponpiveis há cinco anos
atrás, por exemplo), ainda é um desafio juntar e disponibilizar tudo na
*web*.

Além do mais, mesmo quando dados e códigos estão disponíveis, o leitor
ainda tem que baixar os dados, baixar os códigos, e então tentar juntar
tudo, geralmente "a mão". E essa é uma tarefa que geralmente não é fácil
e desencoraja a maioria das pessoas.

Ainda, o leitor pode não ter os mesmos recursos computacionais que o
autor. Por exemplo, o autor pode ter usado um *cluster* para rodar as
análises, que pode não estar disponível para o leitor, e assim
inviabilizar reprodução. Algumas vezes a análise depende de um
componente de *software* que depende de um sistema operacional
específico e isso também pode inviabilizar a reproducibilidade.

Na prática, autores simplesmente "jogam" arquivos na *web*. Existem
jornais que disponibilizam materiais suplementares, mas que sabidamente
são desorganizados ou falhos.

Por esses motivos, a proposta de distribuir dados e códigos
**documentados** de uma pesquisa científica, de maneira acessível e
auto-contida pode ser vista como uma boa alternativa para facilitar a
interação entre autores e leitores.

É nesse sentido que a proposta desse curso foi idealizada: expor as
ferramentas computacionais disponíveis no R para gerar documentos de
análise (dinâmicos), auto-contidos em formato de pacote. Dessa forma,
para reproduzir uma pesquisa é necessário apenas instalar o pacote no R
e reproduzir as análises conforme a documentação. Essa abordagem visa
garantir uma série de benefícios para autores e leitores:

- Os dados e os códigos são disponibilizados conjuntamente
- As análises podem ser documentadas em forma de **vinhetas**, que são
  **documentos dinâmicos** também contidos no pacote
- Eventuais funções que forem criadas para uma análise específica também
  já estão disponíveis com o pacote
- O padrão estrutural (rígido) de criação, formatação e documentação de
    pacotes do R, garante uma **padronização** da maneira como uma
    pesquisa pode ser distribuída de maneira reproduzível
- A dgdg da pesquisa no formato de pacote garante uma facilidade na
  **distribuição**, já que o pacote pode ser disponibilizado no CRAN ou
  em repositórios como o Github
- A compatibilidade das análises com o sistema operacional é aumentada,
  uma vez que os pacotes podem ser criados (a partir do mesmo
  código-fonte) para diferentes sistemas operacionais


# Pesquisa reproduzível

> *The source code is real*
>
> > ESS Project

Pseudo-código para escrever um artigo ou relatório científico:

```{r, results="asis", eval=FALSE}
1. Importe uma base de dados para um *software*
2. Execute um procedimento analítico
3. Gere tabelas e figuras separadamente
4. Copie os resultados de análises
5. Coloque tudo isso em um editor de texto e escreva o documento
```

Mudou alguma linha na base de dados? Esqueceu de algo? Volte para o item
(1) e repita o processo (e boa sorte)!

Pseudo-código para escrever um artigo ou relatório científico
**reprodutível**:

```{r, results="asis", eval=FALSE}
1. Use o R Markdown e realize os 5 passos acima em um único documento
```

Mudou alguma linha na base de dados? Esqueceu de algo? Altere o código e
gere um novo documento.

* Dessa forma, um documento pode ser gerado dinamicamente a partir de um
  **código-fonte**.
* Da mesma forma que um *software* possui seu código-fonte, um documento
  dinâmico é o código-fonte de um relatório.
* É uma combinação de código de computador e as correspondentes
  narrativas descrevendo o resultado que o código está gerando (números,
  tabelas, figuras, ...).
* Quando **compilamos** o documento dinâmico, o código de computador é
  executado, e as saídas são apresentadas. Portanto obtemos um documento
  final que mistura **código** e **texto**.
* Como gerenciamos apenas o código-fonte do documento, ficamos livres de
  todas as etapas manuais mencionadas acima.

# Literate Programming

> *Instead of imagining that our main task is to instruct a computer what
> to do, let us concentrate rather on explaining to humans what we want
> the computer to do.*
>
> > Donald Knuth

O ideia básica por trás de documentos dinâmicos decorre diretamente do
conceito de *literate programming* ("programação letrada"), um paradigma
concebido por [Donald Knuth][] em 1984.

<table>
<tr>
<td align="center"><img src="img/knuth.jpg" height="50%"></td>
<td align="center"><img src="img/Literate_Programming_book_cover.jpg" height="55%"></td>
</tr>
</table>

* O conceito é o de misturar **literatura** (o texto em uma
  linguagem humana) com **códigos de programação**, tornando claro cada
  etapa de um programa e/ou análise
* Com um único código-fonte, podemos
	- Produzir documentos para humanos (HTML, PDF, ...) &rArr; *weave*
	- Produzir documentos para máquinas (código) &rArr; *tangle*
* Knuth criou um sistema chamado **WEB** para fazer essa mistura
  dos seus textos em $TeX$ com a linguagem Pascal
* Atualmente muitos outros sistemas existem para misturar códigos
  com texto em várias linguagens

## Literate Programming no R

Com a ascensão do R no início dos anos 2000, [Friedrich Leisch] criou
o [Sweave] em 2002

* S + weave
* Permite "entrelaçar" textos do $LaTeX$ com códigos do R
* Ainda é muito utilizado e já é distribuído como uma função do R
  dentro do pacote `utils`

No final de 2011, [Yihui Xie][] criou o pacote [knitr][] com
a proposta de ser mais flexível, fácil e **preparado para a Web**

> *knitr = Sweave + cacheSweave + pgfSweave + weaver +
>     animation::saveLatex + R2HTML::RweaveHTML +
>    highlight::HighlightWeaveLatex + 0.2 * brew + 0.1 *
>    SweaveListingUtils + more*

* knit + R
* Uma re-implementação mais moderna do Sweave
* Permite "entrelaçar" textos do $LaTeX$, HTML e **Markdown** com
  códigos do R
* Também permite misturar texto com códigos de outras linguagens:
  Python, awk, C++, shell.
* Adiciona muitas facilidades como
	- Cache
	- Decoração e formatação automática de códigos
	- Geração de gráficos mais direta


<!-- links -->

[Donald Knuth]: https://en.wikipedia.org/wiki/Donald_Knuth
[MathJax]: http://www.mathjax.org
[Dingus]: http://daringfireball.net/projects/markdown/dingus
[Markdown]: http://daringfireball.net/projects/markdown
[rmarkdown]: http://rmarkdown.rstudio.com
[Friedrich Leisch]: http://www.statistik.lmu.de/~leisch
[Sweave]: https://www.statistik.lmu.de/~leisch/Sweave
[Yihui Xie]: http://yihui.name/
[knitr]: http://yihui.name/knitr
[Pandoc]: http://pandoc.org/
[YAML]: http://yaml.org/
[linguagem de marcação]: https://pt.wikipedia.org/wiki/Linguagem_de_marcação/
